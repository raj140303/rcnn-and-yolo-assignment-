{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1689b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is the primary goal of Natural Language Processing (NLP)?\n",
    "# To enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
    "\n",
    "# 2. What does \"tokenization\" refer to in text processing?\n",
    "# Tokenization is the process of breaking down text into smaller units called tokens, such as words or sentences, for easier analysis.\n",
    "\n",
    "# 3. What is the difference between lemmatization and stemming?\n",
    "\n",
    "# Stemming: Cuts off word endings to reduce words to their root form, which may not be a valid word (e.g., \"running\" → \"run\").\n",
    "\n",
    "# Lemmatization: Reduces words to their base or dictionary form (lemma) using vocabulary and morphological analysis (e.g., \"running\" → \"run\").\n",
    "\n",
    "# 4. What is the role of regular expressions (regex) in text processing?\n",
    "# Regex is used to search, match, and manipulate specific patterns of text efficiently, such as extracting email addresses or cleaning unwanted characters.\n",
    "\n",
    "# 5. What is Word2Vec and how does it represent words in a vector space?\n",
    "# Word2Vec is a neural network-based method that represents words as dense vectors in a continuous vector space, capturing semantic relationships between words.\n",
    "\n",
    "# 6. How does frequency distribution help in text analysis?\n",
    "# It helps identify how often each word appears, allowing analysis of common terms, trends, and important keywords in text data.\n",
    "\n",
    "# 7. Why is text normalization important in NLP?\n",
    "# Normalization ensures consistency by converting text to a standard format (e.g., lowercasing, removing punctuation), improving model performance.\n",
    "\n",
    "# 8. What is the difference between sentence tokenization and word tokenization?\n",
    "\n",
    "# Sentence tokenization splits text into sentences.\n",
    "\n",
    "# Word tokenization splits text into words or tokens.\n",
    "\n",
    "# 9. What are co-occurrence vectors in NLP?\n",
    "# Vectors that represent how frequently words appear together in a context window, used to capture word relationships.\n",
    "\n",
    "# 10. What is the significance of lemmatization in improving NLP tasks?\n",
    "# It improves accuracy by grouping different forms of a word into one, reducing sparsity in the data.\n",
    "\n",
    "# 11. What is the primary use of word embeddings in NLP?\n",
    "# To convert words into numerical vectors capturing semantic meaning for machine learning models.\n",
    "\n",
    "# 12. What is an annotator in NLP?\n",
    "# A tool or module that adds metadata like part-of-speech tags or named entities to raw text for further analysis.\n",
    "\n",
    "# 13. What are the key steps in text processing before applying machine learning models?\n",
    "# Tokenization, stopword removal, normalization, stemming/lemmatization, vectorization/embedding.\n",
    "\n",
    "# 14. What is the history of NLP and how has it evolved?\n",
    "# From rule-based systems to statistical models to deep learning and transformer architectures for better language understanding.\n",
    "\n",
    "# 15. Why is sentence processing important in NLP?\n",
    "# Sentences provide context and structure crucial for tasks like parsing, sentiment analysis, and machine translation.\n",
    "\n",
    "# 16. How do word embeddings improve the understanding of language semantics in NLP?\n",
    "# They capture relationships between words based on context, allowing models to understand similarity and analogy.\n",
    "\n",
    "# 17. How does the frequency distribution of words help in text classification?\n",
    "# By highlighting key discriminative words that distinguish between classes.\n",
    "\n",
    "# 18. What are the advantages of using regex in text cleaning?\n",
    "# Efficient pattern matching and extraction of relevant parts of text with minimal code.\n",
    "\n",
    "# 19. What is the difference between Word2Vec and Doc2Vec?\n",
    "\n",
    "# Word2Vec generates embeddings for words.\n",
    "\n",
    "# Doc2Vec generates embeddings for entire documents or sentences.\n",
    "\n",
    "# 20. Why is understanding text normalization important in NLP?\n",
    "# It reduces variability in text, making patterns easier to learn by models.\n",
    "\n",
    "# 21. How does word count help in text analysis?\n",
    "# It provides a simple quantitative measure of term importance and document length.\n",
    "\n",
    "# 22. How does lemmatization help in NLP tasks like search engines and chatbots?\n",
    "# It improves retrieval and understanding by matching words to their base form.\n",
    "\n",
    "# 23. What is the purpose of using Doc2Vec in text processing?\n",
    "# To generate fixed-length vector representations for variable-length documents for classification or clustering.\n",
    "\n",
    "# 24. What is the importance of sentence processing in NLP?\n",
    "# It helps in understanding context, relationships, and intent at a higher level than words.\n",
    "\n",
    "# 25. What is text normalization, and what are the common techniques used in it?\n",
    "# Text normalization standardizes text; common techniques include lowercasing, removing punctuation, and expanding contractions.\n",
    "\n",
    "# 26. Why is word tokenization important in NLP?\n",
    "# It breaks text into meaningful units (words) that are easier to analyze.\n",
    "\n",
    "# 27. How does sentence tokenization differ from word tokenization in NLP?\n",
    "# Sentence tokenization separates text into sentences; word tokenization splits sentences into words.\n",
    "\n",
    "# 28. What is the primary purpose of text processing in NLP?\n",
    "# To convert raw text into a structured format suitable for modeling.\n",
    "\n",
    "# 29. What are the key challenges in NLP?\n",
    "# Ambiguity, context understanding, language variability, sarcasm, and idiomatic expressions.\n",
    "\n",
    "# 30. How do co-occurrence vectors represent relationships between words?\n",
    "# By capturing which words appear near each other in text, indicating semantic or syntactic connections.\n",
    "\n",
    "# 31. What is the role of frequency distribution in text analysis?\n",
    "# Identifies important and frequent words for understanding and feature selection.\n",
    "\n",
    "# 32. What is the impact of word embeddings on NLP tasks?\n",
    "# They improve model accuracy by providing dense, semantically rich input features.\n",
    "\n",
    "# 33. What is the purpose of using lemmatization in text preprocessing?\n",
    "# To reduce word forms to a common base, reducing noise and improving learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0e7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# practical question /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac26d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'helps', 'computers', 'understand', 'human', 'language', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "text = \"NLP helps computers understand human language.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "367ba431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP is amazing.', 'It powers chatbots and voice assistants.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"NLP is amazing. It powers chatbots and voice assistants.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3167cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'helps', 'computers', 'understand', 'human', 'language', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3289758a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"easily\", \"fairly\"]\n",
    "stemmed = [stemmer.stem(word) for word in words]\n",
    "print(stemmed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc027e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'better', 'fly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"better\", \"flies\"]\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914ce8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp is fun\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"NLP, is FUN!!\"\n",
    "normalized = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "print(normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "948b940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp is fun\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "text = \"NLP, is FUN!!\"\n",
    "normalized = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "print(normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb864477",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc2vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc2Vec, TaggedDocument\n\u001b[0;32m      3\u001b[0m documents \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love NLP\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc2Vec converts text to vector\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m tagged \u001b[38;5;241m=\u001b[39m [TaggedDocument(doc\u001b[38;5;241m.\u001b[39msplit(), [i]) \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(documents)]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [\"I love NLP\", \"Doc2Vec converts text to vector\"]\n",
    "tagged = [TaggedDocument(doc.split(), [i]) for i, doc in enumerate(documents)]\n",
    "\n",
    "model = Doc2Vec(tagged, vector_size=50, window=2, min_count=1, epochs=100)\n",
    "print(model.dv[0])  # vector for the first document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd6a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NLP', 'NNP'), ('is', 'VBZ'), ('fun', 'NN'), ('and', 'CC'), ('educational', 'JJ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"NLP is fun and educational\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb4f35e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBarack Obama was born in Hawaii and was the president of the USA.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Barack Obama was born in Hawaii and was the president of the USA.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e6586",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
