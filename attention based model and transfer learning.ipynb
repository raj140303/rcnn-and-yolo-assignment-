{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What is BERT and how does it work?\n",
    "# BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained Transformer-based model designed to understand the context of words in a sentence by looking at both left and right contexts simultaneously (bidirectional). It is trained on masked language modeling and next sentence prediction tasks, enabling it to capture deep language understanding for various NLP tasks.\n",
    "\n",
    "# 2. What are the main advantages of using the attention mechanism in neural networks?\n",
    "# Allows models to focus on relevant parts of the input sequence dynamically.\n",
    "\n",
    "# Handles long-range dependencies better than RNNs.\n",
    "\n",
    "# Improves interpretability by showing which parts of input influence output.\n",
    "\n",
    "# Enables parallelization during training (in Transformers).\n",
    "\n",
    "# 3. How does the self-attention mechanism differ from traditional attention mechanisms?\n",
    "# Self-attention computes attention scores within the same input sequence (e.g., word-to-word in a sentence).\n",
    "\n",
    "# Traditional attention usually computes attention between two different sequences (e.g., encoder-decoder in seq2seq).\n",
    "\n",
    "# Self-attention helps models learn relationships among all tokens in a sequence.\n",
    "\n",
    "# 4. What is the role of the decoder in a Seq2Seq model?\n",
    "# The decoder generates the output sequence step-by-step, often using previous outputs and the encoded representation from the encoder. It predicts the next token in the sequence, often with attention over the encoder outputs.\n",
    "\n",
    "# 5. What is the difference between GPT-2 and BERT models?\n",
    "# BERT is bidirectional and designed primarily for understanding tasks (masked language modeling).\n",
    "\n",
    "# GPT-2 is unidirectional (autoregressive) and designed for generating coherent text sequences.\n",
    "\n",
    "# BERT is good for tasks like classification, QA, while GPT-2 excels at text generation.\n",
    "\n",
    "# 6. Why is the Transformer model considered more efficient than RNNs and LSTMs?\n",
    "# Transformers use self-attention allowing parallel processing of tokens, unlike sequential processing in RNNs/LSTMs.\n",
    "\n",
    "# This leads to much faster training and better handling of long dependencies.\n",
    "\n",
    "# 7. Explain how the attention mechanism works in a Transformer model.\n",
    "# Each token generates query (Q), key (K), and value (V) vectors.\n",
    "\n",
    "# Attention scores are computed by dot product of Q and K, scaled, and passed through softmax to get weights.\n",
    "\n",
    "# These weights multiply V to get the attention output for each token, capturing relevant context.\n",
    "\n",
    "# 8. What is the difference between an encoder and a decoder in a Seq2Seq model?\n",
    "# Encoder reads and compresses the input sequence into a fixed representation.\n",
    "\n",
    "# Decoder uses this representation to generate the output sequence, often with attention to encoder outputs.\n",
    "\n",
    "# 9. What is the primary purpose of using the self-attention mechanism in transformers?\n",
    "# To capture dependencies between all tokens in the input sequence simultaneously, regardless of their distance, improving context understanding.\n",
    "\n",
    "# 10. How does the GPT-2 model generate text?\n",
    "# GPT-2 generates text autoregressively by predicting the next token given all previous tokens, using a stack of Transformer decoder layers.\n",
    "\n",
    "# 11. What is the main difference between the encoder-decoder architecture and a simple neural network?\n",
    "# Encoder-decoder architectures process sequences and learn mappings from input sequences to output sequences, while simple neural networks map fixed-size inputs to outputs without sequence modeling.\n",
    "\n",
    "# 12. Explain the concept of “fine-tuning” in BERT.\n",
    "# Fine-tuning is training BERT further on a specific downstream task (like sentiment analysis) by adding a task-specific output layer and updating weights with task data.\n",
    "\n",
    "# 13. How does the attention mechanism handle long-range dependencies in sequences?\n",
    "# Attention computes direct weighted connections between all tokens, so it can relate distant tokens without sequential bottlenecks.\n",
    "\n",
    "# 14. What is the core principle behind the Transformer architecture?\n",
    "# Using self-attention and feed-forward layers to model relationships in sequences, enabling parallel computation and better long-range dependency capture.\n",
    "\n",
    "# 15. What is the role of the \"position encoding\" in a Transformer model?\n",
    "# Since Transformers don't use recurrence or convolution, position encoding injects information about token positions so the model can understand word order.\n",
    "\n",
    "# 16. How do Transformers use multiple layers of attention?\n",
    "# Transformers stack multiple self-attention layers, allowing the model to learn increasingly abstract and complex representations.\n",
    "\n",
    "# 17. What does it mean when a model is described as “autoregressive” like GPT-2?\n",
    "# It generates outputs sequentially, predicting the next token based on previous tokens, conditioning on the sequence generated so far.\n",
    "\n",
    "# 18. How does BERT's bidirectional training improve its performance?\n",
    "# By considering both left and right context simultaneously, BERT understands word meaning in a richer, more nuanced way.\n",
    "\n",
    "# 19. What are the advantages of using the Transformer over RNN-based models in NLP?\n",
    "# Parallel processing\n",
    "\n",
    "# Better long-range dependency modeling\n",
    "\n",
    "# Faster training\n",
    "\n",
    "# State-of-the-art performance on many NLP tasks\n",
    "\n",
    "# 20. What is the attention mechanism’s impact on the performance of models like BERT and GPT-2?\n",
    "# It significantly improves their ability to model context, capture relationships, and generate coherent and contextually relevant text.\n",
    "\n",
    "# 21. How to implement a simple text classification model using LSTM in Keras?\n",
    "# Prepare tokenized input sequences\n",
    "\n",
    "# Create an embedding layer\n",
    "\n",
    "# Add an LSTM layer\n",
    "\n",
    "# Add dense output layer (e.g., sigmoid for binary classification)\n",
    "\n",
    "# Compile and train the model with labeled data\n",
    "\n",
    "# 22. How to generate sequences of text using a Recurrent Neural Network (RNN)?\n",
    "# Train the RNN on text sequences, then seed it with a starting sequence and repeatedly predict next characters/words to generate text.\n",
    "\n",
    "# 23. How to perform sentiment analysis using a simple CNN model?\n",
    "# Tokenize and embed text\n",
    "\n",
    "# Use convolutional layers to capture n-gram features\n",
    "\n",
    "# Apply max-pooling\n",
    "\n",
    "# Flatten and pass through dense layers for classification\n",
    "\n",
    "# 24. How to perform Named Entity Recognition (NER) using spaCy?\n",
    "# Load spaCy’s pre-trained NER model (nlp = spacy.load(\"en_core_web_sm\"))\n",
    "\n",
    "# Pass text to model (doc = nlp(text))\n",
    "\n",
    "# Extract entities (for ent in doc.ents: print(ent.text, ent.label_))\n",
    "\n",
    "# 25. How to implement a simple Seq2Seq model for machine translation using LSTM in Keras?\n",
    "# Use an encoder LSTM to process input sentence\n",
    "\n",
    "# Use a decoder LSTM with attention or teacher forcing to generate output sentence\n",
    "\n",
    "# Train with parallel source-target sentence pairs\n",
    "\n",
    "# 26. How to generate text using a pre-trained transformer model (GPT-2)?\n",
    "# Load GPT-2 from Hugging Face (transformers library)\n",
    "\n",
    "# Use model’s generate() method to produce text from a prompt\n",
    "\n",
    "# 27. How to apply data augmentation for text in NLP?\n",
    "# Techniques include synonym replacement, random insertion/deletion, back-translation, and paraphrasing to create more training examples.\n",
    "\n",
    "# 28. How can you add an Attention Mechanism to a Seq2Seq model?\n",
    "# Add an attention layer that computes alignment scores between decoder state and encoder outputs at each decoding step, then weight encoder outputs accordingly before producing the decoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2c22e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
